{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fe67840",
   "metadata": {},
   "source": [
    "# Data scraping\n",
    "\n",
    "This notebook collects articles from the CBS Sports RSS feeds. Our goal is to go from no data to a set of articles so we may index and query them later.\n",
    "\n",
    "Since CBS sports has an RSS feed for each sports category, we can avoid painful data scraping and instead use the RSS feeds to collect articles. This is a much more efficient way to collect data, as it allows us to focus on the content rather than the structure of the website. Additionally the RSS feed is explicitly meant to be consumed, whereas scraping a website is often against TOS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ecfa79",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fb57d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¡ USING CACHED DATA â€” 02-data-scraping relies on cached data (ðŸ“¦ snapshot 2025-07-22-16-36-20 (auto))\n"
     ]
    }
   ],
   "source": [
    "from momento_buffconf_workshop import NotebookConfiguration\n",
    "\n",
    "config = NotebookConfiguration.for_scraping(run_demos_live=False)\n",
    "config.print_status_banner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdc68063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import hashlib\n",
    "\n",
    "from langchain_community.document_loaders import RSSFeedLoader\n",
    "from langchain_core.documents import Document\n",
    "from tqdm import tqdm\n",
    "\n",
    "from momento_buffconf_workshop import ArticleContent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30521838",
   "metadata": {},
   "source": [
    "## Collect Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3da669",
   "metadata": {},
   "source": [
    "### Choose feeds\n",
    "\n",
    "We will use the [CBS sports RSS feeds](https://www.cbssports.com/xml/rss) to collect articles. The feeds are available for each sports category, such as NFL, NBA, MLB, etc:\n",
    "![RSS feeds](../images/cbs-rss-feeds.png)\n",
    "\n",
    "\n",
    "An example of one such feed is below for college basketball:\n",
    "![College Basketball RSS feed](../images/cbs-rss-feed-basketball.png)\n",
    "\n",
    "Get hands on and check out the raw RSS feeds by clicking the link above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d5486e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbssports_rss_feeds = {\n",
    "    \"general\": \"https://www.cbssports.com/rss/headlines/\",\n",
    "    \"boxing\": \"https://www.cbssports.com/rss/headlines/boxing\",\n",
    "    \"college_basketball\": \"https://www.cbssports.com/rss/headlines/college-basketball\",\n",
    "    \"college_football\": \"https://www.cbssports.com/rss/headlines/college-football\",\n",
    "    \"golf\": \"https://www.cbssports.com/rss/headlines/golf\",\n",
    "    \"masters\": \"https://www.cbssports.com/rss/tag/masters/\",\n",
    "    \"mlb\": \"https://www.cbssports.com/rss/headlines/mlb\",\n",
    "    \"mma\": \"https://www.cbssports.com/rss/headlines/mma\",\n",
    "    \"nba\": \"https://www.cbssports.com/rss/headlines/nba\",\n",
    "    \"nfl\": \"https://www.cbssports.com/rss/headlines/nfl\",\n",
    "    \"nhl\": \"https://www.cbssports.com/rss/headlines/nhl\",\n",
    "    \"soccer\": \"https://www.cbssports.com/rss/headlines/soccer\",\n",
    "    \"tennis\": \"https://www.cbssports.com/rss/headlines/tennis\",\n",
    "    \"wwe\": \"https://www.cbssports.com/rss/headlines/wwe\",\n",
    "    \"betting\": \"https://www.cbssports.com/rss/headlines/betting/\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b872808",
   "metadata": {},
   "source": [
    "### Fetch and parse\n",
    "\n",
    "This uses [feedparser](https://github.com/kurtmckee/feedparser) to fetch and parse the RSS feeds. It will return a list of articles, then [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) under the hood to parse the HTML content of each article.\n",
    "\n",
    "We use the helper class `ArticleContent` to store the content of each article, including the title, URL, and text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5ee89ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.run_demos_live:\n",
    "    articles: dict[str, list[Document]] = {}\n",
    "    for category, url in tqdm(cbssports_rss_feeds.items()):\n",
    "        loader = RSSFeedLoader(urls=[url], nlp=False)\n",
    "        articles[category] = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d4d9cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.run_demos_live:\n",
    "    ArticleContent(\n",
    "        articles=articles\n",
    "    ).save_json(config.raw_article_path)\n",
    "else:\n",
    "    article_content = ArticleContent.load_json(config.raw_article_path)\n",
    "    articles = article_content.articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8326926b",
   "metadata": {},
   "source": [
    "### Normalize ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a21547b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_url_to_int(url: str) -> str:\n",
    "    return str(int(hashlib.sha256(url.encode(\"utf-8\")).hexdigest()[:16], 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ba5fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_copy = deepcopy(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea7ab748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the hash of the url as the document id\n",
    "\n",
    "for category, docs in article_copy.items():\n",
    "    for doc in docs:\n",
    "        doc.id = hash_url_to_int(doc.metadata[\"link\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "786db9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ArticleContent(\n",
    "    articles=article_copy\n",
    ").save_json(config.normalized_article_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f76f0",
   "metadata": {},
   "source": [
    "### Quick sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a4cad86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 540\n",
      "Number of unique documents: 457\n"
     ]
    }
   ],
   "source": [
    "num_docs = sum(len(docs) for docs in article_copy.values())\n",
    "ids = {doc.id for docs in article_copy.values() for doc in docs}\n",
    "num_unique = len(ids)\n",
    "print(f\"Number of documents: {num_docs}\")\n",
    "print(f\"Number of unique documents: {num_unique}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "momento-buffconf-workshop-py3.11 (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
